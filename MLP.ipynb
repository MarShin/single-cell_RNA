{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, make_scorer, jaccard_similarity_score\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('data/bootstrap.csv', index_col=0)\n",
    "df_e10 = pd.read_csv('data/e10.csv', index_col=0)\n",
    "df_e12 = pd.read_csv('data/e12.csv', index_col=0)\n",
    "# display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [x for x in df.columns if x not in ['label_name', 'label']]\n",
    "X = df[predictors]\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247, 20713) (62, 20713)\n",
      "(247,) (62,)\n"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.ravel()\n",
    "print X_train.shape, X_test.shape\n",
    "print y_train.shape, y_test.shape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)  \n",
    "# display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14295105\n",
      "Iteration 2, loss = 0.65046584\n",
      "Iteration 3, loss = 0.60401362\n",
      "Iteration 4, loss = 0.57365258\n",
      "Iteration 5, loss = 0.54929056\n",
      "Iteration 6, loss = 0.53899132\n",
      "Iteration 7, loss = 0.53150061\n",
      "Iteration 8, loss = 0.52662358\n",
      "Iteration 9, loss = 0.52285236\n",
      "Iteration 10, loss = 0.52000811\n",
      "Iteration 11, loss = 0.51773598\n",
      "Iteration 12, loss = 0.51585732\n",
      "Iteration 13, loss = 0.51421825\n",
      "Iteration 14, loss = 0.51272691\n",
      "Iteration 15, loss = 0.51134856\n",
      "Iteration 16, loss = 0.51003465\n",
      "Iteration 17, loss = 0.50878674\n",
      "Iteration 18, loss = 0.50757931\n",
      "Iteration 19, loss = 0.50640886\n",
      "Iteration 20, loss = 0.50522857\n",
      "Iteration 21, loss = 0.50412193\n",
      "Iteration 22, loss = 0.50297886\n",
      "Iteration 23, loss = 0.50187763\n",
      "Iteration 24, loss = 0.50079596\n",
      "Iteration 25, loss = 0.49966843\n",
      "Iteration 26, loss = 0.49861235\n",
      "Iteration 27, loss = 0.49751549\n",
      "Iteration 28, loss = 0.49645352\n",
      "Iteration 29, loss = 0.49540105\n",
      "Iteration 30, loss = 0.49427143\n",
      "Iteration 31, loss = 0.49324700\n",
      "Iteration 32, loss = 0.49220763\n",
      "Iteration 33, loss = 0.49113474\n",
      "Iteration 34, loss = 0.49005775\n",
      "Iteration 35, loss = 0.48902989\n",
      "Iteration 36, loss = 0.48793299\n",
      "Iteration 37, loss = 0.48688221\n",
      "Iteration 38, loss = 0.48582861\n",
      "Iteration 39, loss = 0.48476491\n",
      "Iteration 40, loss = 0.48371177\n",
      "Iteration 41, loss = 0.48265958\n",
      "Iteration 42, loss = 0.48164815\n",
      "Iteration 43, loss = 0.48059180\n",
      "Iteration 44, loss = 0.47958630\n",
      "Iteration 45, loss = 0.47859463\n",
      "Iteration 46, loss = 0.47758084\n",
      "Iteration 47, loss = 0.47655740\n",
      "Iteration 48, loss = 0.47558445\n",
      "Iteration 49, loss = 0.47457875\n",
      "Iteration 50, loss = 0.47358263\n",
      "Iteration 51, loss = 0.47258198\n",
      "Iteration 52, loss = 0.47158070\n",
      "Iteration 53, loss = 0.47059214\n",
      "Iteration 54, loss = 0.46956746\n",
      "Iteration 55, loss = 0.46859088\n",
      "Iteration 56, loss = 0.46754272\n",
      "Iteration 57, loss = 0.46654844\n",
      "Iteration 58, loss = 0.46553121\n",
      "Iteration 59, loss = 0.46453723\n",
      "Iteration 60, loss = 0.46350142\n",
      "Iteration 61, loss = 0.46251972\n",
      "Iteration 62, loss = 0.46151084\n",
      "Iteration 63, loss = 0.46050371\n",
      "Iteration 64, loss = 0.45953386\n",
      "Iteration 65, loss = 0.45855746\n",
      "Iteration 66, loss = 0.45754589\n",
      "Iteration 67, loss = 0.45654498\n",
      "Iteration 68, loss = 0.45558702\n",
      "Iteration 69, loss = 0.45458435\n",
      "Iteration 70, loss = 0.45362520\n",
      "Iteration 71, loss = 0.45263948\n",
      "Iteration 72, loss = 0.45165348\n",
      "Iteration 73, loss = 0.45069286\n",
      "Iteration 74, loss = 0.44970395\n",
      "Iteration 75, loss = 0.44876478\n",
      "Iteration 76, loss = 0.44777055\n",
      "Iteration 77, loss = 0.44679014\n",
      "Iteration 78, loss = 0.44583858\n",
      "Iteration 79, loss = 0.44484810\n",
      "Iteration 80, loss = 0.44386987\n",
      "Iteration 81, loss = 0.44293289\n",
      "Iteration 82, loss = 0.44195503\n",
      "Iteration 83, loss = 0.44097604\n",
      "Iteration 84, loss = 0.44003636\n",
      "Iteration 85, loss = 0.43904854\n",
      "Iteration 86, loss = 0.43810015\n",
      "Iteration 87, loss = 0.43713504\n",
      "Iteration 88, loss = 0.43616426\n",
      "Iteration 89, loss = 0.43520866\n",
      "Iteration 90, loss = 0.43423151\n",
      "Iteration 91, loss = 0.43330069\n",
      "Iteration 92, loss = 0.43235001\n",
      "Iteration 93, loss = 0.43139948\n",
      "Iteration 94, loss = 0.43048003\n",
      "Iteration 95, loss = 0.42953419\n",
      "Iteration 96, loss = 0.42861277\n",
      "Iteration 97, loss = 0.42770665\n",
      "Iteration 98, loss = 0.42677632\n",
      "Iteration 99, loss = 0.42582272\n",
      "Iteration 100, loss = 0.42489237\n",
      "Iteration 101, loss = 0.42399109\n",
      "Iteration 102, loss = 0.42304014\n",
      "Iteration 103, loss = 0.42211825\n",
      "Iteration 104, loss = 0.42116526\n",
      "Iteration 105, loss = 0.42024118\n",
      "Iteration 106, loss = 0.41929766\n",
      "Iteration 107, loss = 0.41832799\n",
      "Iteration 108, loss = 0.41738132\n",
      "Iteration 109, loss = 0.41645944\n",
      "Iteration 110, loss = 0.41547359\n",
      "Iteration 111, loss = 0.41453641\n",
      "Iteration 112, loss = 0.41357572\n",
      "Iteration 113, loss = 0.41265396\n",
      "Iteration 114, loss = 0.41168846\n",
      "Iteration 115, loss = 0.41075516\n",
      "Iteration 116, loss = 0.40981835\n",
      "Iteration 117, loss = 0.40892206\n",
      "Iteration 118, loss = 0.40797633\n",
      "Iteration 119, loss = 0.40710007\n",
      "Iteration 120, loss = 0.40618769\n",
      "Iteration 121, loss = 0.40529958\n",
      "Iteration 122, loss = 0.40439340\n",
      "Iteration 123, loss = 0.40350420\n",
      "Iteration 124, loss = 0.40261331\n",
      "Iteration 125, loss = 0.40167189\n",
      "Iteration 126, loss = 0.40079865\n",
      "Iteration 127, loss = 0.39987165\n",
      "Iteration 128, loss = 0.39898481\n",
      "Iteration 129, loss = 0.39807288\n",
      "Iteration 130, loss = 0.39719145\n",
      "Iteration 131, loss = 0.39630744\n",
      "Iteration 132, loss = 0.39540164\n",
      "Iteration 133, loss = 0.39451784\n",
      "Iteration 134, loss = 0.39363072\n",
      "Iteration 135, loss = 0.39275809\n",
      "Iteration 136, loss = 0.39190680\n",
      "Iteration 137, loss = 0.39103242\n",
      "Iteration 138, loss = 0.39016146\n",
      "Iteration 139, loss = 0.38928203\n",
      "Iteration 140, loss = 0.38843267\n",
      "Iteration 141, loss = 0.38756570\n",
      "Iteration 142, loss = 0.38667854\n",
      "Iteration 143, loss = 0.38585545\n",
      "Iteration 144, loss = 0.38494922\n",
      "Iteration 145, loss = 0.38412843\n",
      "Iteration 146, loss = 0.38329595\n",
      "Iteration 147, loss = 0.38245235\n",
      "Iteration 148, loss = 0.38162770\n",
      "Iteration 149, loss = 0.38079459\n",
      "Iteration 150, loss = 0.37996652\n",
      "Iteration 151, loss = 0.37914790\n",
      "Iteration 152, loss = 0.37830036\n",
      "Iteration 153, loss = 0.37747380\n",
      "Iteration 154, loss = 0.37662991\n",
      "Iteration 155, loss = 0.37580573\n",
      "Iteration 156, loss = 0.37495490\n",
      "Iteration 157, loss = 0.37415994\n",
      "Iteration 158, loss = 0.37333618\n",
      "Iteration 159, loss = 0.37251130\n",
      "Iteration 160, loss = 0.37170967\n",
      "Iteration 161, loss = 0.37090106\n",
      "Iteration 162, loss = 0.37007758\n",
      "Iteration 163, loss = 0.36926840\n",
      "Iteration 164, loss = 0.36846348\n",
      "Iteration 165, loss = 0.36764855\n",
      "Iteration 166, loss = 0.36681854\n",
      "Iteration 167, loss = 0.36600351\n",
      "Iteration 168, loss = 0.36519771\n",
      "Iteration 169, loss = 0.36434797\n",
      "Iteration 170, loss = 0.36355914\n",
      "Iteration 171, loss = 0.36272046\n",
      "Iteration 172, loss = 0.36189533\n",
      "Iteration 173, loss = 0.36109361\n",
      "Iteration 174, loss = 0.36027590\n",
      "Iteration 175, loss = 0.35947313\n",
      "Iteration 176, loss = 0.35863076\n",
      "Iteration 177, loss = 0.35783370\n",
      "Iteration 178, loss = 0.35704647\n",
      "Iteration 179, loss = 0.35619041\n",
      "Iteration 180, loss = 0.35540138\n",
      "Iteration 181, loss = 0.35461426\n",
      "Iteration 182, loss = 0.35381794\n",
      "Iteration 183, loss = 0.35301933\n",
      "Iteration 184, loss = 0.35224086\n",
      "Iteration 185, loss = 0.35144968\n",
      "Iteration 186, loss = 0.35069924\n",
      "Iteration 187, loss = 0.34990770\n",
      "Iteration 188, loss = 0.34912997\n",
      "Iteration 189, loss = 0.34836011\n",
      "Iteration 190, loss = 0.34758448\n",
      "Iteration 191, loss = 0.34683138\n",
      "Iteration 192, loss = 0.34605699\n",
      "Iteration 193, loss = 0.34527499\n",
      "Iteration 194, loss = 0.34449628\n",
      "Iteration 195, loss = 0.34373391\n",
      "Iteration 196, loss = 0.34292848\n",
      "Iteration 197, loss = 0.34216046\n",
      "Iteration 198, loss = 0.34138377\n",
      "Iteration 199, loss = 0.34059534\n",
      "Iteration 200, loss = 0.33981958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  2.,  2.,  1.,  2.,\n",
       "        2.,  1.,  2.,  2.,  1.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  2.,\n",
       "        2.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  1.,  2.,  1.,  2.,  2.,\n",
       "        2.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy [0.677]\n",
      "Accuracy [0.677]\n",
      "Hamming Loss [0.323]\n",
      "Accuracy Score [0.677]\n",
      "Sim Score [0.677]\n",
      "[ 1.  2.]\n"
     ]
    }
   ],
   "source": [
    "y_predict = clf.predict(X_test)\n",
    "display(y_predict)\n",
    "print(\"Test Accuracy [%0.3f]\" % ((y_test == y_predict).mean()))\n",
    "\n",
    "print(\"Accuracy [%0.3f]\" % ((y_predict==y_test).mean()))\n",
    "print(\"Hamming Loss [%0.3f]\" % ((hamming_loss(y_predict,y_test))))\n",
    "print(\"Accuracy Score [%0.3f]\" % ((accuracy_score(y_predict,y_test))))\n",
    "print(\"Sim Score [%0.3f]\" % ((jaccard_similarity_score(y_predict,y_test))))\n",
    "print np.unique(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.35585694\n",
      "Iteration 2, loss = 1.17314122\n",
      "Iteration 3, loss = 1.17250104\n",
      "Iteration 4, loss = 1.17176827\n",
      "Iteration 5, loss = 1.17106511\n",
      "Iteration 6, loss = 1.17038694\n",
      "Iteration 7, loss = 1.16972906\n",
      "Iteration 8, loss = 1.16909035\n",
      "Iteration 9, loss = 1.16842134\n",
      "Iteration 10, loss = 1.16770723\n",
      "Iteration 11, loss = 1.16708595\n",
      "Iteration 12, loss = 1.16643693\n",
      "Iteration 13, loss = 1.16576312\n",
      "Iteration 14, loss = 1.16512673\n",
      "Iteration 15, loss = 1.16449717\n",
      "Iteration 16, loss = 1.16385257\n",
      "Iteration 17, loss = 1.16317570\n",
      "Iteration 18, loss = 1.16256392\n",
      "Iteration 19, loss = 1.16191193\n",
      "Iteration 20, loss = 1.16128616\n",
      "Iteration 21, loss = 1.16064995\n",
      "Iteration 22, loss = 1.15998731\n",
      "Iteration 23, loss = 1.15942326\n",
      "Iteration 24, loss = 1.15876092\n",
      "Iteration 25, loss = 1.15814972\n",
      "Iteration 26, loss = 1.15752631\n",
      "Iteration 27, loss = 1.15690330\n",
      "Iteration 28, loss = 1.15628992\n",
      "Iteration 29, loss = 1.15569497\n",
      "Iteration 30, loss = 1.15513551\n",
      "Iteration 31, loss = 1.15452710\n",
      "Iteration 32, loss = 1.15395311\n",
      "Iteration 33, loss = 1.15338402\n",
      "Iteration 34, loss = 1.15283214\n",
      "Iteration 35, loss = 1.15225057\n",
      "Iteration 36, loss = 1.15168926\n",
      "Iteration 37, loss = 1.15115829\n",
      "Iteration 38, loss = 1.15063704\n",
      "Iteration 39, loss = 1.15007993\n",
      "Iteration 40, loss = 1.14955322\n",
      "Iteration 41, loss = 1.14900315\n",
      "Iteration 42, loss = 1.14852450\n",
      "Iteration 43, loss = 1.14795791\n",
      "Iteration 44, loss = 1.14745662\n",
      "Iteration 45, loss = 1.14694513\n",
      "Iteration 46, loss = 1.14641522\n",
      "Iteration 47, loss = 1.14593884\n",
      "Iteration 48, loss = 1.14541567\n",
      "Iteration 49, loss = 1.14493928\n",
      "Iteration 50, loss = 1.14446827\n",
      "Iteration 51, loss = 1.14395424\n",
      "Iteration 52, loss = 1.14346778\n",
      "Iteration 53, loss = 1.14301458\n",
      "Iteration 54, loss = 1.14256734\n",
      "Iteration 55, loss = 1.14205710\n",
      "Iteration 56, loss = 1.14162989\n",
      "Iteration 57, loss = 1.14119797\n",
      "Iteration 58, loss = 1.14072776\n",
      "Iteration 59, loss = 1.14027662\n",
      "Iteration 60, loss = 1.13982418\n",
      "Iteration 61, loss = 1.13944136\n",
      "Iteration 62, loss = 1.13896146\n",
      "Iteration 63, loss = 1.13853822\n",
      "Iteration 64, loss = 1.13812650\n",
      "Iteration 65, loss = 1.13767844\n",
      "Iteration 66, loss = 1.13729076\n",
      "Iteration 67, loss = 1.13684633\n",
      "Iteration 68, loss = 1.13646260\n",
      "Iteration 69, loss = 1.13603482\n",
      "Iteration 70, loss = 1.13556791\n",
      "Iteration 71, loss = 1.13523970\n",
      "Iteration 72, loss = 1.13481270\n",
      "Iteration 73, loss = 1.13440652\n",
      "Iteration 74, loss = 1.13401345\n",
      "Iteration 75, loss = 1.13364879\n",
      "Iteration 76, loss = 1.13320007\n",
      "Iteration 77, loss = 1.13284961\n",
      "Iteration 78, loss = 1.13248197\n",
      "Iteration 79, loss = 1.13212015\n",
      "Iteration 80, loss = 1.13170942\n",
      "Iteration 81, loss = 1.13135023\n",
      "Iteration 82, loss = 1.13099304\n",
      "Iteration 83, loss = 1.13059155\n",
      "Iteration 84, loss = 1.13027710\n",
      "Iteration 85, loss = 1.12991830\n",
      "Iteration 86, loss = 1.12955688\n",
      "Iteration 87, loss = 1.12919203\n",
      "Iteration 88, loss = 1.12888472\n",
      "Iteration 89, loss = 1.12851729\n",
      "Iteration 90, loss = 1.12818048\n",
      "Iteration 91, loss = 1.12784373\n",
      "Iteration 92, loss = 1.12750338\n",
      "Iteration 93, loss = 1.12721463\n",
      "Iteration 94, loss = 1.12689690\n",
      "Iteration 95, loss = 1.12655934\n",
      "Iteration 96, loss = 1.12624444\n",
      "Iteration 97, loss = 1.12593749\n",
      "Iteration 98, loss = 1.12564621\n",
      "Iteration 99, loss = 1.12531165\n",
      "Iteration 100, loss = 1.12501785\n",
      "Iteration 101, loss = 1.12476796\n",
      "Iteration 102, loss = 1.12442863\n",
      "Iteration 103, loss = 1.12414707\n",
      "Iteration 104, loss = 1.12383582\n",
      "Iteration 105, loss = 1.12355932\n",
      "Iteration 106, loss = 1.12328913\n",
      "Iteration 107, loss = 1.12299321\n",
      "Iteration 108, loss = 1.12268657\n",
      "Iteration 109, loss = 1.12242239\n",
      "Iteration 110, loss = 1.12215410\n",
      "Iteration 111, loss = 1.12185188\n",
      "Iteration 112, loss = 1.12162740\n",
      "Iteration 113, loss = 1.12129993\n",
      "Iteration 114, loss = 1.12107551\n",
      "Iteration 115, loss = 1.12080447\n",
      "Iteration 116, loss = 1.12053738\n",
      "Iteration 117, loss = 1.12029047\n",
      "Iteration 118, loss = 1.12001761\n",
      "Iteration 119, loss = 1.11977510\n",
      "Iteration 120, loss = 1.11954831\n",
      "Iteration 121, loss = 1.11924499\n",
      "Iteration 122, loss = 1.11903745\n",
      "Iteration 123, loss = 1.11878345\n",
      "Iteration 124, loss = 1.11853187\n",
      "Iteration 125, loss = 1.11828846\n",
      "Iteration 126, loss = 1.11804931\n",
      "Iteration 127, loss = 1.11779640\n",
      "Iteration 128, loss = 1.11755460\n",
      "Iteration 129, loss = 1.11735095\n",
      "Iteration 130, loss = 1.11708103\n",
      "Iteration 131, loss = 1.11686570\n",
      "Iteration 132, loss = 1.11663944\n",
      "Iteration 133, loss = 1.11639438\n",
      "Iteration 134, loss = 1.11617067\n",
      "Iteration 135, loss = 1.11594869\n",
      "Iteration 136, loss = 1.11576844\n",
      "Iteration 137, loss = 1.11550254\n",
      "Iteration 138, loss = 1.11532559\n",
      "Iteration 139, loss = 1.11508921\n",
      "Iteration 140, loss = 1.11489457\n",
      "Iteration 141, loss = 1.11471234\n",
      "Iteration 142, loss = 1.11449524\n",
      "Iteration 143, loss = 1.11430903\n",
      "Iteration 144, loss = 1.11410993\n",
      "Iteration 145, loss = 1.11392850\n",
      "Iteration 146, loss = 1.11374122\n",
      "Iteration 147, loss = 1.11355814\n",
      "Iteration 148, loss = 1.11337670\n",
      "Iteration 149, loss = 1.11320235\n",
      "Iteration 150, loss = 1.11303023\n",
      "Iteration 151, loss = 1.11281896\n",
      "Iteration 152, loss = 1.11266223\n",
      "Iteration 153, loss = 1.11249316\n",
      "Iteration 154, loss = 1.11231761\n",
      "Iteration 155, loss = 1.11215124\n",
      "Iteration 156, loss = 1.11196345\n",
      "Iteration 157, loss = 1.11183175\n",
      "Iteration 158, loss = 1.11166265\n",
      "Iteration 159, loss = 1.11148635\n",
      "Iteration 160, loss = 1.11131702\n",
      "Iteration 161, loss = 1.11117731\n",
      "Iteration 162, loss = 1.11102940\n",
      "Iteration 163, loss = 1.11086692\n",
      "Iteration 164, loss = 1.11073271\n",
      "Iteration 165, loss = 1.11057071\n",
      "Iteration 166, loss = 1.11041955\n",
      "Iteration 167, loss = 1.11027279\n",
      "Iteration 168, loss = 1.11013778\n",
      "Iteration 169, loss = 1.10998728\n",
      "Iteration 170, loss = 1.10981941\n",
      "Iteration 171, loss = 1.10970570\n",
      "Iteration 172, loss = 1.10953875\n",
      "Iteration 173, loss = 1.10939942\n",
      "Iteration 174, loss = 1.10928887\n",
      "Iteration 175, loss = 1.10910570\n",
      "Iteration 176, loss = 1.10899140\n",
      "Iteration 177, loss = 1.10884801\n",
      "Iteration 178, loss = 1.10872249\n",
      "Iteration 179, loss = 1.10858086\n",
      "Iteration 180, loss = 1.10843877\n",
      "Iteration 181, loss = 1.10830764\n",
      "Iteration 182, loss = 1.10820522\n",
      "Iteration 183, loss = 1.10806583\n",
      "Iteration 184, loss = 1.10791110\n",
      "Iteration 185, loss = 1.10780985\n",
      "Iteration 186, loss = 1.10768544\n",
      "Iteration 187, loss = 1.10753935\n",
      "Iteration 188, loss = 1.10741174\n",
      "Iteration 189, loss = 1.10730094\n",
      "Iteration 190, loss = 1.10715314\n",
      "Iteration 191, loss = 1.10702487\n",
      "Iteration 192, loss = 1.10692872\n",
      "Iteration 193, loss = 1.10679918\n",
      "Iteration 194, loss = 1.10667230\n",
      "Iteration 195, loss = 1.10654679\n",
      "Iteration 196, loss = 1.10645230\n",
      "Iteration 197, loss = 1.10633717\n",
      "Iteration 198, loss = 1.10620844\n",
      "Iteration 199, loss = 1.10609891\n",
      "Iteration 200, loss = 1.10599941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict_proba(df_e12.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163],\n",
       "       [ 0.32788779,  0.38220058,  0.28991163]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['progenitor', 'neuron', 'adipo.dermal', 'myogenic'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e12_labels = pd.read_csv('data/e12_labels.csv', index_col=0)\n",
    "df_e12_labels.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels [1,2,3] = 'bulk_neuralTube1', 'bulk_BAT1', 'bulk_muscle1 = mygenic'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
